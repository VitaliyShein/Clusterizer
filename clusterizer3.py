# -*- coding: utf-8 -*-
"""Clusterizer3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BmqVrNRektDk563onVVSwqvl6Qao998j
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install textract
import textract

# Commented out IPython magic to ensure Python compatibility.
# %%python -m pip install pip<24.1

# Commented out IPython magic to ensure Python compatibility.
# %pip install langdetect

from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sentence_transformers import SentenceTransformer
import numpy as np
import shutil
import os
from sklearn.preprocessing import Normalizer

def get_embeddings(model_name='all-MiniLM-L6-v2'):
    """
    Преобразует список текстов в эмбеддинги

    Args:
        texts (list): Список текстовых строк
        model_name (str): Название предобученной модели (по умолчанию 'all-MiniLM-L6-v2')

    Returns:
        np.array: Массив эмбеддингов размером (n_texts, embedding_dim)
    """
    texts = []
    model = SentenceTransformer(model_name)

    # Путь к папке с файлами
    folder_path = "/content/drive/MyDrive/TrainData"

    # Поддерживаемые форматы
    supported_extensions = ['.pdf', '.docx', '.txt', '.pptx', '.jpg', '.png', '.jpeg']

    # Проходим по всем файлам в папке
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)

        # Проверяем расширение файла
        if os.path.isfile(file_path) and any(file_path.lower().endswith(ext) for ext in supported_extensions):
            try:
              print(f"Обработка файла: {filename}...")
              text = textract.process(file_path).decode('utf-8')
              texts.append(text)
            except ValueError:
              print("ERROR")
    embeddings = model.encode(texts, convert_to_numpy=True)

    # Нормализация L2 (делает длину векторов = 1)
    normalizer = Normalizer(norm='l2')
    embeddings_normalized = normalizer.transform(embeddings)

    # Проверка
    print(np.linalg.norm(embeddings_normalized[0]))  # Должно быть 1.0
    return embeddings_normalized

def get_ONE_embedding(text, model_name='all-MiniLM-L6-v2'):
    """
    Преобразует список текстов в эмбеддинги

    Args:
        texts (list): Список текстовых строк
        model_name (str): Название предобученной модели (по умолчанию 'all-MiniLM-L6-v2')

    Returns:
        np.array: Массив эмбеддингов размером (n_texts, embedding_dim)
    """
    model = SentenceTransformer(model_name)
    ONE_embedding = model.encode([text], convert_to_numpy=True)

    # Нормализация L2 (делает длину векторов = 1)
    normalizer = Normalizer(norm='l2')
    ONE_embedding_normalized = normalizer.transform(ONE_embedding)

    # Проверка
    print(np.linalg.norm(ONE_embedding_normalized[0]))  # Должно быть 1.0

    return ONE_embedding_normalized

def train_cluster_model(embeddings, n_clusters=3, random_state=42):
    """
    Обучает модель кластеризации на эмбеддингах

    Args:
        embeddings (np.array): Массив эмбеддингов
        n_clusters (int): Количество кластеров
        random_state (int): Для воспроизводимости

    Returns:
        KMeans: Обученная модель K-Means
    """
    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)
    kmeans.fit(embeddings)
    return kmeans

def predict_cluster(text, kmeans_model):
    """
    Предсказывает кластер для нового текста

    Args:
        text (str): Текст для классификации
        kmeans_model: Обученная модель кластеризации
        embedding_model_name (str): Модель для эмбеддингов

    Returns:
        int: Номер кластера (0, 1, 2, ...)
    """
    embedding = get_ONE_embedding(text)
    cluster = kmeans_model.predict(embedding)
    print("Predicted cluster is ", cluster[0])
    return cluster[0]

"""Загрузка BERT"""

from transformers import BertTokenizer, pipeline
import torch

# Для английского
en_summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Для русского
ru_summarizer = pipeline(
    "summarization",
    model="IlyaGusev/rut5_base_sum_gazeta",
    tokenizer="IlyaGusev/mbart_ru_sum_gazeta"
)

def summarize_text(text, summarizer):
    """Генерация краткого описания"""
    try:
        summary = summarizer(
          text[:2000] if len(text) > 20 else text,
          max_length=15,
          min_length=3,
          length_penalty=0.2  # Делаем суммаризацию короче
        )        # Для очень длинных текстов берем первые 1000 токенов
        return summary[0]['summary_text']
    except Exception as e:
        print(f"Summarization error: {e}")
        return "No summary available"

from langdetect import detect

def detect_language(text):
    try:
        return detect(text)
    except:
        return "unknown"

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# Путь к папке с файлами
folder_path = "/content/drive/MyDrive/InputData"

# Поддерживаемые форматы
supported_extensions = ['.pdf', '.docx', '.txt', '.pptx', '.jpg', '.png', '.jpeg']

# Проходим по всем файлам в папке
for filename in os.listdir(folder_path):
    file_path = os.path.join(folder_path, filename)

    # Проверяем расширение файла
    if os.path.isfile(file_path) and any(file_path.lower().endswith(ext) for ext in supported_extensions):
        try:
          print(f"Обработка файла: {filename}...")

          # Извлекаем текст
          text = textract.process(file_path).decode('utf-8')

          # get_embeddings(text)

          # Определяем язык
          text_lang = detect_language(text)


          # Определение краткого описания текста
          if (text_lang == "ru"):
            text_description = summarize_text(text, ru_summarizer)
            print(text_description)
          elif (text_lang == "en"):
            text_description = summarize_text(text, en_summarizer)
            print(text_description)

          # Переименование
          shutil.move(file_path, folder_path+"/"+text_description)
        except ValueError:
          print("ERROR")

embeddings = get_embeddings()

kmeans_model = train_cluster_model(embeddings)

def move_to_folder(cluster):

  directory = "/content/drive/MyDrive/InputData"
 # Вызов соответствующей функции обработки
  if cluster == 0:
    source_file = filename
    destination_path = "/content/drive/MyDrive/0"

    # Создать папку, если её нет
    if not os.path.exists(destination_path):
        os.makedirs(destination_path)
    begPath = directory + '/' + filename
    shutil.copy(begPath, '/content/drive/MyDrive/0')  #  os.rename(source_file, destination_path)
    print(f"Файл перемещён в {destination_path}")

  elif cluster == 1:
    source_file = filename
    destination_path = "/content/drive/MyDrive/1"

    # Создать папку, если её нет
    if not os.path.exists(destination_path):
        os.makedirs(destination_path)
    begPath = directory + '/' + filename
    shutil.copy(begPath, '/content/drive/MyDrive/1')  #  os.rename(source_file, destination_path)
    print(f"Файл перемещён в {destination_path}")

  elif cluster == 2:
    source_file = filename
    destination_path = "/content/drive/MyDrive/2"

    # Создать папку, если её нет
    if not os.path.exists(destination_path):
        os.makedirs(destination_path)
    begPath = directory + '/' + filename
    shutil.copy(begPath, '/content/drive/MyDrive/2')  #  os.rename(source_file, destination_path)
    print(f"Файл перемещён в {destination_path}")
  else:
        print("Unknown document type")

# Путь к папке с файлами
folder_path = "/content/drive/MyDrive/InputData"

# Поддерживаемые форматы
supported_extensions = ['.pdf', '.docx', '.txt', '.pptx', '.jpg', '.png', '.jpeg']

# Проходим по всем файлам в папке
for filename in os.listdir(folder_path):
    file_path = os.path.join(folder_path, filename)

    # Проверяем расширение файла
    if os.path.isfile(file_path) and any(file_path.lower().endswith(ext) for ext in supported_extensions):
        try:
          print(f"Обработка файла: {filename}...")
           # Извлекаем текст
          text = textract.process(file_path).decode('utf-8')

          # Находим в какой кластер попадает текст
          cluster = predict_cluster(text, kmeans_model)

          # Перемещаем файл в папку соответствующего кластера
          move_to_folder(cluster)



        except ValueError:
          print("ERROR")

"""Определение кластера текста с помощью BERT"""